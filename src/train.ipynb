{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 데이터셋 생성"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import mediapipe as mp\n",
    "import numpy as np\n",
    "import time, os\n",
    "\n",
    "# 학습시킬 데이터 지정\n",
    "actions = ['Next', 'Preview', 'Cam_Off', 'Cam_On']\n",
    "seq_length = 30 # window의 사이즈\n",
    "secs_for_action = 30 # 하나의 제스쳐를 찍는데 걸리는 시간\n",
    "\n",
    "# MediaPipe hands model\n",
    "\n",
    "# 초기화\n",
    "mp_hands = mp.solutions.hands\n",
    "mp_drawing = mp.solutions.drawing_utils\n",
    "hands = mp_hands.Hands(\n",
    "    max_num_hands = 1, # 몇 개의 손을 인식할 것인지\n",
    "    min_detection_confidence = 0.5,\n",
    "    min_tracking_confidence = 0.5)\n",
    "\n",
    "# 웹캠 열기\n",
    "cap = cv2.VideoCapture(0)\n",
    "dirpath = './data/Mini_Project/motion/'\n",
    "created_time = int(time.time())\n",
    "os.makedirs(dirpath+'dataset', exist_ok=True) # 데이터 셋을 저장할 폴더 만들기\n",
    "\n",
    "# 웹캠을 열어서 데이터 모으기\n",
    "while cap.isOpened():\n",
    "    for idx, action in enumerate(actions):\n",
    "        data = []\n",
    "\n",
    "        # 이미지 읽기\n",
    "        ret, img = cap.read()\n",
    "\n",
    "        # flip, 웹캠 이미지가 거울처럼 나타나기 때문\n",
    "        img = cv2.flip(img, 1)\n",
    "\n",
    "        # 어떤 제스쳐를 학습시킬 것인지 표시\n",
    "        cv2.putText(img, f'Waiting for collecting {action.upper()} action...', org=(10,30),\n",
    "        fontFace=cv2.FONT_HERSHEY_SIMPLEX, fontScale=1, color=(255,255,255), thickness=2)\n",
    "        \n",
    "        #3초동안 대기\n",
    "        cv2.imshow('img', img)\n",
    "        cv2.waitKey(3000)\n",
    "\n",
    "        start_time = time.time()\n",
    "        \n",
    "        #30초동안 촬영\n",
    "        while time.time() - start_time < secs_for_action:\n",
    "            ret, img = cap.read()\n",
    "\n",
    "            img = cv2.flip(img, 1)\n",
    "            img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB) # opencv는 기본적으로 BGR로 영상을 읽기 때문에 RGB로 바꿔야 함\n",
    "            result = hands.process(img) # RGB로 바꾼 데이터를 result에 저장\n",
    "            img = cv2.cvtColor(img, cv2.COLOR_RGB2BGR)\n",
    "\n",
    "            if result.multi_hand_landmarks is not None:\n",
    "                for res in result.multi_hand_landmarks:\n",
    "                    joint = np.zeros((21,4))\n",
    "                    for j,lm in enumerate(res.landmark):\n",
    "                        joint[j] = [lm.x, lm.y, lm.z, lm.visibility] # 각 점의 x, y, z 좌표 & 점이 이미지 상에서 보이는지 안 보이는지\n",
    "\n",
    "                    # 점들 간의 각도 계산하기\n",
    "                    v1 = joint[[0,1,2,3,0,5,6,7,0,9,10,11,0,13,14,15,0,17,18,19], :3] # Parent joint\n",
    "                    v2 = joint[[1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20], :3] # Child joint\n",
    "                    v = v2 - v1 # v2와 v1 사이의 벡터 구하기\n",
    "\n",
    "                    # 벡터 정규화 시키기(단위 벡터 구하기)\n",
    "                    v = v / np.linalg.norm(v, axis=1)[:, np.newaxis]\n",
    "\n",
    "                    # 점곱을 구한 다음 arccos으로 각도 구하기\n",
    "                    angle = np.arccos(np.einsum('nt,nt->n',\n",
    "                        v[[0,1,2,4,5,6,8,9,10,12,13,14,16,17,18],:], \n",
    "                        v[[1,2,3,5,6,7,9,10,11,13,14,15,17,18,19],:])) # [15,]\n",
    "\n",
    "                    angle = np.degrees(angle) # 라디안을 각도로 바꾸기\n",
    "\n",
    "                    angle_label = np.array([angle], dtype=np.float32)\n",
    "                    angle_label = np.append(angle_label, idx) # 라벨 추가\n",
    "\n",
    "                    d = np.concatenate([joint.flatten(), angle_label])\n",
    "\n",
    "                    data.append(d)\n",
    "\n",
    "                    mp_drawing.draw_landmarks(img, res, mp_hands.HAND_CONNECTIONS) # 랜드마크 그리기\n",
    "            \n",
    "            cv2.imshow('img', img)\n",
    "            if cv2.waitKey(1) == ord('q'):\n",
    "                break\n",
    "\n",
    "        data = np.array(data)\n",
    "        print(action, data.shape)\n",
    "        np.save(os.path.join(dirpath+'dataset', f'raw_{action}_{created_time}'),data)\n",
    "\n",
    "        # 시퀀스 데이터로 변환\n",
    "        full_seq_data = []\n",
    "        for seq in range(len(data) - seq_length):\n",
    "            full_seq_data.append(data[seq:seq + seq_length])\n",
    "\n",
    "        full_seq_data = np.array(full_seq_data)\n",
    "        print(action, full_seq_data.shape)\n",
    "        np.save(os.path.join(dirpath+'dataset', f'seq_{action}_{created_time}'), full_seq_data)\n",
    "    break\n",
    "\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 학습"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "from tensorflow import keras\n",
    "from keras.utils import to_categorical\n",
    "\n",
    "folder_path = './data/Mini_Project/motion/dataset/'\n",
    "\n",
    "actions = ['Next', 'Preview', 'Cam_Off', 'Cam_On']\n",
    "\n",
    "# \"seq\"가 포함된 파일 리스트를 가져옵니다.\n",
    "file_list = [file for file in os.listdir(folder_path) if 'seq' in file] \n",
    "# actions 리스트 순서대로 정렬\n",
    "file_list.sort(key=lambda x: [actions.index(a) for a in actions if a in x]) \n",
    "\n",
    "sequences = []\n",
    "for file_name in file_list:\n",
    "    file_path = os.path.join(folder_path, file_name)\n",
    "    sequence = np.load(file_path)\n",
    "    sequences.append(sequence)\n",
    "\n",
    "data = np.concatenate(sequences, axis=0)\n",
    "print(file_list)\n",
    "print(data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data의 마지막 값이 라벨이므로 x_data와 labels로 나눈기\n",
    "x_data = data[:, :, :-1]\n",
    "labels = data[:, 0, -1]\n",
    "\n",
    "print(x_data.shape)\n",
    "print(labels.shape)\n",
    "\n",
    "# One-hot 인코딩\n",
    "y_data = to_categorical(labels, num_classes=len(actions))\n",
    "print(y_data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "x_data = x_data.astype(np.float32)\n",
    "y_data = y_data.astype(np.float32)\n",
    "\n",
    "x_train, x_val, y_train, y_val = train_test_split(x_data, y_data, test_size=0.2, random_state=42)\n",
    "\n",
    "print(x_train.shape, y_train.shape)\n",
    "print(x_val.shape, y_val.shape)\n",
    "\n",
    "from keras.models import Sequential\n",
    "from keras.layers import LSTM, Dense\n",
    "                    # input_shape = [30, 99], 30 : 윈도우의 크기, 99 : 랜드마크, visibility, 각도\n",
    "model = Sequential([LSTM(64, activation='relu', input_shape=x_train.shape[1:3]),\n",
    "                    Dense(32, activation='relu'),\n",
    "                    Dense(len(actions), activation='softmax')])\n",
    "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.callbacks import ModelCheckpoint, ReduceLROnPlateau\n",
    "\n",
    "history = model.fit(x_train, \n",
    "                    y_train, \n",
    "                    validation_data=(x_val, y_val),\n",
    "                    epochs=50,\n",
    "                    callbacks=[ModelCheckpoint(dirpath+'dataset/models/model.h5', monitor='val_accuracy', verbose=1, save_best_only=True, mode='auto'),\n",
    "                               ReduceLROnPlateau(monitor='val_accuracy', factor=0.5, patience=14, verbose=1, mode='auto')])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "fig, loss_ax = plt.subplots(figsize=(8, 5))\n",
    "acc_ax = loss_ax.twinx()\n",
    "\n",
    "loss_ax.plot(history.history['loss'], 'y', label='train loss')\n",
    "loss_ax.plot(history.history['val_loss'], 'r', label='val loss')\n",
    "loss_ax.set_xlabel('epoch')\n",
    "loss_ax.set_ylabel('loss')\n",
    "loss_ax.legend(loc='upper left')\n",
    "\n",
    "acc_ax.plot(history.history['accuracy'], 'b', label='train acc')\n",
    "acc_ax.plot(history.history['val_accuracy'], 'g', label='val acc')\n",
    "acc_ax.set_ylabel('accuracy')\n",
    "acc_ax.legend(loc='upper left')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 테스트"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import mediapipe as mp\n",
    "import numpy as np\n",
    "from tensorflow import keras\n",
    "from keras.models import load_model\n",
    "\n",
    "actions = ['Next', 'Preview', 'Cam_Off', 'Cam_On']\n",
    "seq_length = 30\n",
    "\n",
    "model = load_model('./data/Mini_Project/motion/dataset/models/model.h5')\n",
    "\n",
    "# MediaPipe hands model (초기화)\n",
    "mp_hands = mp.solutions.hands\n",
    "# mp_drawing = mp.solutions.drawing_utils\n",
    "\n",
    "hands = mp_hands.Hands(\n",
    "    max_num_hands = 1,\n",
    "    min_detection_confidence=0.9,\n",
    "    min_tracking_confidence=0.9)\n",
    "\n",
    "# 웹캠 열기\n",
    "cap = cv2.VideoCapture(0)\n",
    "\n",
    "seq = []\n",
    "action_seq = []\n",
    "\n",
    "while cap.isOpened():\n",
    "    ret, img = cap.read()\n",
    "    img0 = img.copy()\n",
    "\n",
    "    img = cv2.flip(img, 1)\n",
    "    img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "    result = hands.process(img)\n",
    "    img = cv2.cvtColor(img, cv2.COLOR_RGB2BGR)\n",
    "\n",
    "    if result.multi_hand_landmarks is not None:\n",
    "        for res in result.multi_hand_landmarks:\n",
    "            joint = np.zeros((21,4))\n",
    "            for j, lm in enumerate(res.landmark):\n",
    "                joint[j] = [lm.x, lm.y, lm.z, lm.visibility]\n",
    "\n",
    "            # 점들 간의 각도 계산하기\n",
    "            v1 = joint[[0,1,2,3,0,5,6,7,0,9,10,11,0,13,14,15,0,17,18,19], :3] # Parent joint\n",
    "            v2 = joint[[1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20], :3] # Child joint\n",
    "            v = v2 - v1 # v2와 v1 사이의 벡터 구하기\n",
    "\n",
    "            # 점곱을 구한 다음 arccos으로 각도 구하기\n",
    "            v = v / np.linalg.norm(v, axis=1)[:, np.newaxis]\n",
    "\n",
    "            # Get angle using arcos of dot product\n",
    "            angle = np.arccos(np.einsum('nt,nt->n',\n",
    "                v[[0,1,2,4,5,6,8,9,10,12,13,14,16,17,18],:], \n",
    "                v[[1,2,3,5,6,7,9,10,11,13,14,15,17,18,19],:])) # [15,]\n",
    "\n",
    "            angle = np.degrees(angle) # 라디안을 각도로 바꾸기\n",
    "\n",
    "            d = np.concatenate([joint.flatten(), angle])\n",
    "\n",
    "\n",
    "            seq.append(d)\n",
    "\n",
    "            mp_drawing.draw_landmarks(img, res, mp_hands.HAND_CONNECTIONS)\n",
    "\n",
    "            if len(seq) < seq_length:\n",
    "                continue\n",
    "\n",
    "            input_data = np.expand_dims(np.array(seq[-seq_length:], dtype=np.float32), axis=0)\n",
    "\n",
    "            # 모델 예측\n",
    "            y_pred = model.predict(input_data).squeeze()\n",
    "\n",
    "            # 예측한 값의 인덱스 구하기\n",
    "            i_pred = int(np.argmax(y_pred))\n",
    "            conf = y_pred[i_pred]\n",
    "\n",
    "            # confidence가 0.9보다 작으면\n",
    "            if conf < 0.99:\n",
    "                continue # 제스쳐 인식 못 한 상황으로 판단\n",
    "\n",
    "            action = actions[i_pred]\n",
    "            action_seq.append(action) # action_seq에 action을 저장\n",
    "            #print(action_seq)\n",
    "            # 보인 제스쳐의 횟수가 3 미만인 경우에는 계속\n",
    "            if len(action_seq) < 3:\n",
    "                continue\n",
    "            # 제스쳐 판단 불가이면 this_action은 ?\n",
    "            this_action = '?'\n",
    "            # 만약 마지막 3개의 제스쳐가 같으면 제스쳐가 제대로 취해졌다고 판단\n",
    "            if action_seq[-1] == action_seq[-2] == action_seq[-3]:\n",
    "                this_action = action\n",
    "                print(this_action)\n",
    "            # 텍스트 출력\n",
    "            cv2.putText(img, f'{this_action.upper()}', org=(int(res.landmark[0].x * img.shape[1]), \n",
    "                                                            int(res.landmark[0].y * img.shape[0] + 20)), \n",
    "                                                            fontFace=cv2.FONT_HERSHEY_SIMPLEX, fontScale=1, \n",
    "                                                            color=(255, 255, 255), thickness=2)\n",
    "    # out.write(img0)\n",
    "    # out2.write(img)\n",
    "    cv2.imshow('img', img)\n",
    "    if cv2.waitKey(1) == ord('q'):\n",
    "        break\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
