{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ultralytics import YOLO\n",
    "import mediapipe as mp\n",
    "import cv2\n",
    "import torch\n",
    "import numpy as np\n",
    "import glob\n",
    "import pandas as pd\n",
    "from tensorflow import keras\n",
    "from keras.models import load_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# 세그먼트를 위한 YOLOv8 모델 로드\n",
    "model = YOLO('yolov8n-seg.pt')\n",
    "# 모션인식을 위한 모델 로드\n",
    "gesture = load_model('./data/Mini_Project/motion/dataset/models/model.h5')\n",
    "\n",
    "# MediaPipe hands model (초기화)\n",
    "mp_hands = mp.solutions.hands\n",
    "mp_drawing = mp.solutions.drawing_utils\n",
    "\n",
    "hands = mp_hands.Hands(\n",
    "    max_num_hands = 1,\n",
    "    min_detection_confidence=0.5,\n",
    "    min_tracking_confidence=0.5)\n",
    "\n",
    "gesture_start = False  # 제스처 인식 시작 여부를 나타내는 변수\n",
    "gesture_end = False  # 제스처 인식 종료 여부를 나타내는 변수\n",
    "gesture_duration = 0  # 제스처 인식 지속 시간을 나타내는 변수 (프레임 수)\n",
    "is_gesture_detected = False  # 제스처 중첩 인식 방지를 위한 변수\n",
    "\n",
    "# 배경을 불러오기\n",
    "folder_path = './data/background'  # 폴더 경로 설정\n",
    "file_list = glob.glob(folder_path + '/*')  # 폴더 내의 파일 목록 얻기\n",
    "file_count = len(file_list)  # 파일 개수 계산\n",
    "idx = 0 # 파일 리스트를 적절히 초기화해야 함\n",
    "actions = ['Next', 'Preview', 'Cam_Off', 'Cam_On'] # 모션 인식 레이블\n",
    "seq_length = 30\n",
    "seq = []\n",
    "action_seq = []\n",
    "# 웹캠을 위한 비디오 캡처 객체 열기\n",
    "cap = cv2.VideoCapture(0)\n",
    "\n",
    "first_hand_detection = True\n",
    "\n",
    "\n",
    "# 비디오 프레임을 반복하여 처리\n",
    "while cap.isOpened():\n",
    "    # start = time.time()\n",
    "    # 비디오에서 프레임 읽기\n",
    "    success, frame = cap.read()\n",
    "    hand_frame = frame.copy()\n",
    "    hand_frame = cv2.cvtColor(hand_frame, cv2.COLOR_RGB2BGR)\n",
    "    # ret, img = cap.read()\n",
    "    # img0 = img.copy()\n",
    "\n",
    "    # hand_frame = cv2.flip(hand_frame, 1)\n",
    "    # hand_frame = cv2.cvtColor(hand_frame, cv2.COLOR_BGR2RGB)\n",
    "    # result = hands.process(img)\n",
    "    # img = cv2.cvtColor(img, cv2.COLOR_RGB2BGR)\n",
    "\n",
    "    if success:\n",
    "        # YOLOv8를 사용하여 프레임에 대한 추론 실행\n",
    "        # classes를 통해 사람만 탐지하도록 설정\n",
    "        results = model(frame, classes=0, verbose=False)\n",
    "        handresult = hands.process(hand_frame)\n",
    "        # handresult = cv\n",
    "        \n",
    "        # 검정색 배경 이미지 생성\n",
    "        background = np.zeros(frame.shape, dtype=np.uint8)\n",
    "        blackscreen = np.zeros(frame.shape, dtype=np.uint8)\n",
    "        \n",
    "        try: # 사람이 탐지되지않으면 오류나는것을 방지\n",
    "            for result in results:\n",
    "                # print(result.names) # 탐지 객체 목록 딕셔너리\n",
    "                # if result.boxes.conf >= 0.80:\n",
    "                    # print(result.boxes.conf)\n",
    "                    # if result.probs is not None and result.probs.top1 == 1:\n",
    "                    for mask in result.masks:\n",
    "                        # 마스크 데이터에서 차원 축소\n",
    "                        m = torch.squeeze(mask.data)\n",
    "                        # 마스크를 RGB 형식으로 변환하여 컴포지트(composite) 텐서 생성\n",
    "                        composite = torch.stack((m, m, m), 2)\n",
    "                        # 프레임과 컴포지트를 곱하여 마스크 영역 추출\n",
    "                        tmp = frame * composite.cpu().numpy().astype(np.uint8)\n",
    "                        # 추출된 마스크 영역을 배경에 누적\n",
    "                        # 미리 생성해둔 검은 배경에 세그먼트된 영역을 채워넣음\n",
    "                        background += tmp\n",
    "                # else:\n",
    "                #     # results = model(frame, classes=0)\n",
    "        except:\n",
    "            pass\n",
    "        \n",
    "        if handresult.multi_hand_landmarks is not None:\n",
    "            for res in handresult.multi_hand_landmarks:\n",
    "                joint = np.zeros((21, 4))\n",
    "                for j, lm in enumerate(res.landmark):\n",
    "                    joint[j] = [lm.x, lm.y, lm.z, lm.visibility]\n",
    "                    \n",
    "            # 점들 간의 각도 계산하기\n",
    "            v1 = joint[[0,1,2,3,0,5,6,7,0,9,10,11,0,13,14,15,0,17,18,19], :3] # Parent joint\n",
    "            v2 = joint[[1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20], :3] # Child joint\n",
    "            v = v2 - v1 # v2와 v1 사이의 벡터 구하기\n",
    "\n",
    "            # 점곱을 구한 다음 arccos으로 각도 구하기\n",
    "            v = v / np.linalg.norm(v, axis=1)[:, np.newaxis]\n",
    "            \n",
    "            # Get angle using arcos of dot product\n",
    "            angle = np.arccos(np.einsum('nt,nt->n',\n",
    "                v[[0,1,2,4,5,6,8,9,10,12,13,14,16,17,18],:], \n",
    "                v[[1,2,3,5,6,7,9,10,11,13,14,15,17,18,19],:])) # [15,]\n",
    "            angle = np.degrees(angle) # 라디안을 각도로 바꾸기\n",
    "\n",
    "            d = np.concatenate([joint.flatten(), angle])\n",
    "\n",
    "            seq.append(d)\n",
    "            # mp_drawing.draw_landmarks(frame, res, mp_hands.HAND_CONNECTIONS)\n",
    "\n",
    "            if len(seq) < seq_length:\n",
    "                continue\n",
    "            input_data = np.expand_dims(np.array(seq[-seq_length:], dtype=np.float32), axis=0)\n",
    "            # 모델 예측\n",
    "            y_pred = gesture.predict(input_data).squeeze()\n",
    "            # 예측한 값의 인덱스 구하기\n",
    "            i_pred = int(np.argmax(y_pred))\n",
    "            conf = y_pred[i_pred]\n",
    "            # confidence가 0.9보다 작으면\n",
    "            if conf < 0.7:\n",
    "                continue # 제스쳐 인식 못 한 상황으로 판단\n",
    "            action = actions[i_pred]\n",
    "            action_seq.append(action) # action_seq에 action을 저장\n",
    "            #print(action_seq)\n",
    "            # 보인 제스쳐의 횟수가 3 미만인 경우에는 계속\n",
    "            if len(action_seq) < 3:\n",
    "                continue\n",
    "            # 만약 마지막 3개의 제스쳐가 같으면 제스쳐가 제대로 취해졌다고 판단\n",
    "            if action_seq[-1] == action_seq[-2] == action_seq[-3]:\n",
    "                this_action = action\n",
    "                # # print(this_action)\n",
    "                if action == 'Next':\n",
    "                    if not is_gesture_detected:  # 중첩 인식 방지 변수 확인\n",
    "                        idx += 1\n",
    "                        if idx >= len(file_list):\n",
    "                            idx = 0\n",
    "                        is_gesture_detected = True  # 중첩 인식 방지 변수 설정\n",
    "                elif action == 'Preview':\n",
    "                    if not is_gesture_detected:  # 중첩 인식 방지 변수 확인\n",
    "                        idx -= 1\n",
    "                        if idx < 0:\n",
    "                            idx = len(file_list) - 1\n",
    "                        is_gesture_detected = True  # 중첩 인식 방지 변수 설정\n",
    "                elif action == 'Cam_off':\n",
    "                    cv2.imshow(\"Project\", blackscreen)\n",
    "                    is_gesture_detected = True  # 중첩 인식 방지 변수 설정\n",
    "                elif action == 'cam_On':\n",
    "                    cv2.imshow(\"Project\", cv2.flip(background, 1)) # 좌우반전\n",
    "                    is_gesture_detected = True  # 중첩 인식 방지 변수 설정\n",
    "                else:\n",
    "                    gesture_start = False  # 제스처 인식 시작 여부를 나타내는 변수\n",
    "                    gesture_end = False  # 제스처 인식 종료 여부를 나타내는 변수\n",
    "                    gesture_duration = 0  # 제스처 인식 지속 시간을 나타내는 변수 (프레임 수)\n",
    "                    is_gesture_detected = False  # 제스처 중첩 인식 방지를 위한 변수\n",
    "                # elif action == 'Cam_Off':\n",
    "                #     background_copy = background\n",
    "                #     background = bgimg_resized\n",
    "                # elif action == 'Cam_On':\n",
    "                    # background = background_copy\n",
    "            \n",
    "        # 배경 이미지를 원하는 배경으로 마스킹\n",
    "        bgimg = cv2.imread(file_list[idx])\n",
    "        bgimg_resized = cv2.resize(bgimg, (frame.shape[1], frame.shape[0]))\n",
    "        # 검정색배경에 세그먼트된 영상에서 0인값을 설정해둔 배경으로 마스킹\n",
    "        background = np.where(background == 0, bgimg_resized, background)\n",
    "        # background = cv2.resize(background, (960,720))\n",
    "        background = background.astype(np.uint8)\n",
    "        # 마스킹된 이미지 출력\n",
    "        if first_hand_detection:\n",
    "            if handresult.multi_hand_landmarks is None:\n",
    "                cv2.imshow(\"Project\", blackscreen)\n",
    "            else:\n",
    "                first_hand_detection = False\n",
    "                cv2.imshow(\"Project\", cv2.flip(background, 1)) # 좌우반전\n",
    "        else:\n",
    "            cv2.imshow(\"Project\", cv2.flip(background, 1)) # 좌우반전\n",
    "\n",
    "        key = cv2.waitKey(1)\n",
    "        # 'q', 'esc', x버튼 키가 눌리면 루프 종료\n",
    "        if key == 113 or key == 27 or cv2.getWindowProperty('Project', cv2.WND_PROP_VISIBLE) < 1: # ord(\"q\")\n",
    "            break\n",
    "        # 'c' 키가 눌리면 다음 배경\n",
    "        elif key == 99: # ord(\"c\")\n",
    "            idx += 1\n",
    "            if idx >= len(file_list):\n",
    "                idx = 0\n",
    "        # 'z' 키가 눌리면 이전 배경\n",
    "        elif key == 122: # ord(\"z\")\n",
    "            idx -= 1\n",
    "            if idx < 0:\n",
    "                idx = len(file_list) - 1\n",
    "        # end = time.time()\n",
    "        # print(end - start)\n",
    "    else:\n",
    "        # 비디오의 끝에 도달하면 루프 종료\n",
    "        break\n",
    "\n",
    "# 비디오 캡처 객체 해제 및 화면 창 닫기\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
